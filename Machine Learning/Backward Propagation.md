# Backward Propagation

Backpropagation is the process of updating weights of the model. It is done by computing partial derivates of each parameter for gradient descent.

Backpropagation, short for "backward propagation of errors," is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights.

![image](https://github.com/powerswing/TBA/assets/73081144/76bd7a3d-30dd-412f-9756-9872e04980c9)

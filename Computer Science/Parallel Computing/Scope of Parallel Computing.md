# Scope of Parallel Computing

- Bit-Level Parallelism: This type of parallel computing is based upon increasing processor word size. In short, increasing the word size minimizes the number of instructions that the processor must execute to complete an operation on a variable whose length is longer than that of the word.
- Instruction-Level Parallelism: This form of parallel computing is a measure of how many instructions a processor can manage to execute simultaneously. The two primary types of instruction-level parallel computing are hardware and software. The hardware approach operates upon a dynamic parallelism, whereas the software approach operates upon a static parallelism. 
- Data Parallelism: In contrast, data computing parallelism is the operation of multiple processors across various parallel computing environments. In other words, this parallelization distributes data across many processors, where the data are then worked on simultaneously. This is different from the aforementioned types of parallel computing, where a single processor worked to increase its individual efficiency working with data.
- Task Parallelism: Similar to data computing parallelism, task parallelism also requires multiple processors across parallel computing environments. In this case, however, tasks are distributed to these processors. The main difference between task parallelism and data parallelism is that task parallelism focuses on running multiple tasks simultaneously on the same data, whereas the latter focuses on running a singular task on the data across different processors.

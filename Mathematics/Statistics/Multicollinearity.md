# Multicollinearity

Perfectly correlated regressors, also known as multicollinearity, occurs when two or more independent variables in a regression analysis are highly correlated with each other. In this case, the regression model cannot accurately estimate the separate effects of each independent variable on the dependent variable because they are essentially measuring the same underlying concept.

When two or more independent variables are perfectly correlated, it is impossible to determine which independent variable is causing changes in the dependent variable. This can lead to unstable or unreliable estimates of the regression coefficients, which can result in incorrect inferences and predictions.

Perfectly correlated regressors can also lead to inflated standard errors, which in turn can reduce the statistical significance of the regression coefficients, making it difficult to detect significant relationships between the independent variables and the dependent variable.

To detect multicollinearity, researchers often use statistical tests such as the variance inflation factor (VIF) or correlation matrix to identify highly correlated independent variables. If multicollinearity is present, it can be addressed by removing one or more of the correlated independent variables from the regression model or by using methods such as principal component analysis (PCA) or partial least squares (PLS) regression to reduce the dimensionality of the data.
